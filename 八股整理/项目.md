自我介绍：面试官，您好，我是徐秋晨，目前是电子科技大学研三的一名学生，专业是通信工程。

主要做了两个和C++网络编程的项目：

第一个项目是在学习网络编程和在github中参考其他优秀的开源项目中完成的基于C++11实现的多线程web服务器，解析http请求，并发模型是基于主从reactor模型和I/O多路复用epoll实现的，MainReactor 只有一个，负责响应 client 的连接请求，并建立连接，它使用一个NIO Selector。在建立连接后用轮询的方式分配给某个SubReactor，因为涉及到跨线程任务分配，需要加锁，这里的锁由某个特定线程中的 loop 创建，只会被该线程和主线程竞争。SubReactor 可以有一个或者多个，每个 subReactor 都会在一个独立线程中运行，并且维护一个独立的 NIO Selector。当主线程把新连接分配给了某个 SubReactor，该线程此时可能正阻塞在多路选择器(epoll)的等待中，使用了 eventfd 进行异步唤醒，线程会从 epoll_wait中醒来，得到活跃事件，进行处理。后端日志主要是由多个缓冲区构成的，参考了muduo 介绍了“双缓冲区”的思想，一个主要的，另一个防止第一个写满了没地方写，写满或者时间到了就和另外两个交换**指针**，然后把满的往文件里写。

第二个项目是在第一个项目的基础上，研究了异步 I/O 相较于同步 I/O 的优势，但异步 I/O 在编写代码的过程中较为复杂，同时 Linux 中对于异步 I/O 不是特别完善，`aio` 系列函数是定义的异步操作接口，而不是操作系统级别支持的，一般的基于 Linux 下的高性能网络程序都是使用 Reactor 方案。因此引入了协程概念，通过调研 C++ 中的开源协程库和 Golang 中的协程模型设计的基于 C++11 的轻量级对称协程网络库，协程是轻量级的线程实现，其调度完全由开发者进行控制，协程是在用户态中实现的调度，相比于线程，避免了内核态的上下文切换造成了性能损失，协程的切换需要保存当前的上下文，首先是参考了云风的 coroutine 的协程库，使用了 glibc 中的 uncontext 函数集实现了上下文的协程，封装了协程对象和协程调度器，其中每一个线程对应一个 Processor 实例，协程 Coroutine 实例运行在 Processor 的主循环中，Processor 使用 epoller 和定时器 timer 进行任务调度。而 Scheduler 则并不存在一个循环，它是一个全局单例，当某个线程中调用 `co_go()` 运行一个新协程后，实际会调用该实例的方法，选择一个协程最少的 Processor 接管新的协程，当然，用户也可以指定具体某一个 Processor 来接管新的协程；同时参考了 STL 二级空间配置器的设计，实现了对象池，参考 STL 二级空间配置器的设计，对象池每次创建对象时，会先从**内存池**中取出相应大小的块，内存池与对象大小强相关的，有一个空闲链表，每次分配空间都从空间链表中取，如果空闲链表没有内容，首先会分配 `(分配次数+40)*对象大小` 的空间，然后分成一个个的块，挂到空闲链表上。从内存池取出所需内存块后，会判断对象是否拥有 non-trivial （显式定义默认构造、拷贝构造）构造函数，没有的话直接返回，有的话使用 placement new 构造对象。同时对原生的 socket 进行了协程化改造和是使用 C++11 中 stomic 实现了自旋锁，在测试后相比于上一个主从 reactor 模型 QPS 要提升1.5倍左右。

# 网络编程

## 协程目的

在传统的J2EE系统中都是基于每个请求占用一个线程去完成完整的业务逻辑（包括事务）。所以系统的吞吐能力取决于每个线程的操作耗时。如果遇到很耗时的I/O行为，则整个系统的吞吐立刻下降，因为这个时候线程一直处于阻塞状态，如果线程很多的时候，会存在很多线程处于空闲状态（等待该线程执行完才能执行），造成了资源应用不彻底。协程适用于被阻塞的，且需要大量并发的场景。但不适用于大量计算的多线程，遇到此种情况，更好实用线程去解决。

## 实现基于 TCP/IP 的客户端服务端

**TCP 服务端默认函数调用顺序**

- `socket()` 创建套接字
- `bind()` 分配套接字地址
- `listen()` 等待连接请求状态
- `accept()` 允许连接
- `read()/write()` 交换数据
- `close()` 断开连接

**TCP 客户端默认函数调用顺序**

- `socket()` 创建套接字
- `connect()` 请求连接
- `read()/write()` 交换数据
- `close()` 断开连接

**注意**

- 服务器端创建套接字后连续调用 bind、listen 函数进入等待状态，客户端通过 connect 函数发起连接请求
- 客户端只能等到服务端调用 listen 后才能调用 connect 函数
- 客户端调用 connect 函数前，服务器可能率先调用 accept 函数，然后服务端进入阻塞状态，直到客户端调用 connect 为止

**TCP/IP 套接字中的 I/O 缓冲**

write 函数调用后不是立即传输数据，read 函数调用后也不是马上接收数据，而是将这些数据转移到输入和输出缓冲中。

I/O 缓冲有以下特点：

- I/O 缓冲在每个TCP套接字中单独存在
- I/O 缓冲在创建套接字时自动生成
- 即使关闭套接字也会继续传递输出缓冲中遗留的数据
- 关闭套接字将丢失输入缓冲中的数据

## 什么是 I/O

所谓的 I/O 就是输入和输出，也可以理解为读和写，针对不同的对象，I/O 模型可以分为磁盘 I/O 和 网络 I/O 模型。

I/O 操作会涉及到用户空间和内核空间的转换，先来理解以下规则：

- 内存空间分为用户空间和内核空间，也称为用户缓冲区和用户缓冲区；
- 用户的应用程序不能直接操作内核空间，需要将数据从内核空间拷贝到用户空间才能使用；
- 无论是 read 操作，还是 write 操作，都只能在内核空间里执行；
- 磁盘 I/O 和网络 I/O 请求加载到内存的数据都是先放在内核空间的。

所谓的读和写操作：

- **读操作：**操作系统检查内核缓冲区有没有需要的数据，如果内核缓冲区就有需要的数据，就把内核空间的数据 copy 到用户空间，供用户的应用程序使用。如果没有相应的数据，对于磁盘 I/O，直接从磁盘中读取到内核缓冲区（这个过程不需要 CPU 参与）；对于网络 I/O，应用数据等待客户端发送数据，如果客户端还没有发送数据，对应的应用程序将阻塞，直到客户端发送了数据，应用程序才会被唤醒，从 Socket 协议找中读取客户端发送的数据到内核空间，然后把内核空间的数据 copy 到用户空间，供应用程序使用。
- **写操作：**用户应用程序将用户空间 copy 到内核空间的缓冲区中（如果没有，需要从磁盘 --> 内核缓冲区 --> 用户缓冲区依次读取），这时对用户程序来说写操作已经完成，对于什么时候再写到磁盘中或者通过网络发出去，由操作系统决定。

## I/O 模型

- 阻塞 I/O
- 非阻塞 I/O
- I/O 多路复用（select, poll, epoll）
- 异步 I/O

## I/O 多路复用

### **select/poll**

select 函数将许多个文件描述符集中到一起进行监视，使用 fd_set 数组保存被监视的文件描述符的变化，这个数组是以位存储在内核中的。即该数组所在的索引就是对应文件描述符的id。

select 实现多路复用的方式是，将已连接的 Socket 都放到一个**文件描述符集合**，然后调用 select 函数将文件描述符集合**拷贝**到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过**遍历**文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合**拷贝**回用户态里，然后用户态还需要再通过**遍历**的方法找到可读或可写的 Socket，然后再对其处理。

对于 select 这种方式，需要进行 **2 次「遍历」文件描述符集合**，一次是在内核态里，一个次是在用户态里 ，而且还会发生 **2 次「拷贝」文件描述符集合**，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。

select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 `1024`，只能监听 0~1023 的文件描述符；poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。

两种方式没有本质的区别，**都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长。

### **epoll**

epoll 的函数：

```cpp
int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout);
```

1. 调用epoll_create建立一个epoll对象(在epoll文件系统中给这个句柄分配资源)；
2. 调用epoll_ctl向epoll对象中添加套接字；
3. 调用epoll_wait收集发生事件的连接。

epoll 高效的原因主要是 epoll_wait 这个函数。由于我们在调用 epoll_create 时，内核除了帮我们在 epoll 文件系统里建了个 file 结点，在内核 cache 里建了个红黑树用于存储以后 epoll_ctl 传来的 fd，这些 fd 其实已经在内核态了，当你再次调用 epoll_wait 时，不需要再拷贝进内核态（select需要再全部拷贝到内核态）。还会再建立一个双向链表，用于存储准备就绪的事件，list 链表的维护：当我们执行 epoll_ctl 时，会把对应 fd 放到红黑树中，还会给内核终端处理程序注册一个回调函数。如果这个句柄的中断到了，就把它放在list链表中去。被 epoll_wait 调用时，就去看这个链表是不是为空，若不为空就返回，为空就等待指定的事件再返回。相比较于 select 省去了将数组搬迁到内核，以及在内核中遍历数组，和将遍历结果拷贝到用户态。而且 select 使用的是数组的遍历查找状态的改变进而使用位操作函数，epoll 通过红黑树查找，是通过事件注册可以检测出是读事件，写事件，还是异常事件。当有大量连接但是其中只有一少部分处于活跃状态就会影响到性能，epoll 可以处理高并发理论上连接没有限制，

注意使用的时候有一个惊群问题，epoll 的工作模式又分为水平触发和边沿触发，默认情况下使用水平触发通俗理解为可以读多次数据，边沿触发理解为只能读一次一次把数据读完，既然是一次读完会不会存在阻塞式读取呢？当然可以选择使用不阻塞读取防止出现阻塞读取而影响后续的任务安排，select 使用数组记录事件，连接之前已经有确切的事件范围。

**水平触发和边缘触发**

- 在 LT（水平触发）模式下，只要这个文件描述符还有数据可读，每次 epoll_wait 都会返回它的事件，提醒用户程序去操作。
- 在 ET（边缘触发）模式下，在它检测到有 I/O 事件时，通过 epoll_wait 调用会得到有事件通知的文件描述符，**对于每一个被通知的文件描述符，如可读，则必须将该文件描述符一直读到空**，让 errno 返回 EAGAIN 为止，否则下次的 epoll_wait 不会返回余下的数据，会丢掉事件。如果 ET 模式不是非阻塞的，那这个一直读或一直写势必会在最后一次阻塞。

**水平触发和边缘触发的应用场景**

LT 的编程会比 ET 的编程更简洁的场景，对于可读事件，ET 模式下的编程需要 read 到 EAGAIN 位置,发来的数据量多且并发量大的时候,还可能造成其他事件的饥饿，需要在应用层再额外代码以保证及时响应，而 LT 可直接每个消息事件 read 固定大小以保证每个连接公平(不管是单线程还是多线程模型)，数据量大的且没读完的下次还会继续触发；对于写事件，ET 会实现更简单高效。例子：要 write 1M 的数据，缓冲区只有 2kb，需要用 `epoll_wait()` 可写事件 EPOLLOUT，对于 ET 模式，直接写完后等就可以了，如果是 LT 模式，写完后，还需要再调用一次 `epoll_ctl` 来删去 EPOLLOUT 事件，否则下次调用 `epoll_wait` 还是会继续触发返回可写事件。

如果是并发量比较大，而且每个连接通信的数据量比较大的情况，为了不饥饿掉其他连接，采用 LT 触发模式注册可读事件，通信框架中限制每个连接接受的最大数据为 1M，假设一个连接中总共会有5M，那当可读事件触发时，就只会读取1M，然后把这份数据缓存在应用层，然后处理其他连接后再回来处理接下来的第二个1M，重复上述逻辑，知道5M数据都取完毕，再把这些数据批量抛给业务逻辑；这样保证了每个业务的吞吐量；2.如果对每个用户实时性要求比较高，就每次尽最大努力读完5M数据后再处理其他可读事件

### epoll 是同步还是异步的？

- 从 IO 层面来看，epoll 绝对是同步的；
- 从消息处理层面来看，epoll 是异步的.

select，poll，epoll 本质上都是同步 I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的（可能通过 while 循环来检测内核将数据准备的怎么样了， 而不是属于内核的一种通知用户态机制），仍然需要 read、write 去读写数据。

用户线程定期轮询 epoll 文件描述符上的事件，事件发生后，读取事件对应的 epoll_data，该结构中包含了文件fd和数据地址，由于采用了 mmap，程序可以直接读取数据（epoll_wait 函数）。有人把 epoll 这种方式叫做同步非阻塞（NIO），因为用户线程需要不停地轮询，自己读取数据，看上去好像只有一个线程在做事情。也有人把这种方式叫做异步非阻塞（AIO），因为毕竟是内核线程负责扫描 fd 列表，并填充事件链表的。个人认为真正理想的异步非阻塞，应该是内核线程填充事件链表后，主动通知用户线程，或者调用应用程序事先注册的回调函数来处理数据，如果还需要用户线程不停的轮询来获取事件信息，就不是太完美了，所以也有不少人认为 epoll 是伪 AIO，还是有道理的。

**epoll 事件**

- EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；
- EPOLLOUT：表示对应的文件描述符可以写；
- EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；
- EPOLLERR：表示对应的文件描述符发生错误；
- EPOLLHUP：表示对应的文件描述符被挂断；
- EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。
- EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里

## 两种处理事件方式

如果要让服务器服务多个客户端，那么最直接的方式就是为每一条连接创建线程。其实创建进程也是可以的，原理是一样的，进程和线程的区别在于线程比较轻量级些，线程的创建和线程间切换的成本要小些，为了描述简述，后面都以线程为例。处理完业务逻辑后，随着连接关闭后线程也同样要销毁了，但是这样不停地创建和销毁线程，不仅会带来性能开销，也会造成浪费资源，而且如果要连接几万条连接，创建几万个线程去应对也是不现实的。要这么解决这个问题呢？我们可以使用「资源复用」的方式。也就是不用再为每个连接创建线程，而是创建一个「线程池」，将连接分配给线程，然后一个线程可以处理多个连接的业务。不过，这样又引来一个新的问题，线程怎样才能高效地处理多个连接的业务？

当一个连接对应一个线程时，线程一般采用「read -> 业务处理 -> send」的处理流程，如果当前连接没有数据可读，那么线程会阻塞在 `read` 操作上（ socket 默认情况是阻塞 I/O），不过这种阻塞方式并不影响其他线程。但是引入了线程池，那么一个线程要处理多个连接的业务，线程在处理某个连接的 `read` 操作时，如果遇到没有数据可读，就会发生阻塞，那么线程就没办法继续处理其他连接的业务。要解决这一个问题，最简单的方式就是将 socket 改成非阻塞，然后线程不断地轮询调用 `read` 操作来判断是否有数据，这种方式虽然该能够解决阻塞的问题，但是解决的方式比较粗暴，因为轮询是要消耗 CPU 的，而且随着一个 线程处理的连接越多，轮询的效率就会越低。

**Reactor 模式**

**定义：**是一种接收多个输入事件的服务器事件驱动处理模式。服务器端通过 IO 多路复用来处理这些输入事件，并将这些事件同步分派给对应的处理线程。Reactor 模式本质上就是将收到的事件进行分发处理。Reactor 模式中有两个关键的组成：

- 主反应堆 Reactor 在一个单独的线程中运行，负责监听和分发事件，将接收到的事件分为监听 socket 和连接 socket，连接 socket 放入任务队列让线程池线程去抢占式调度。

- Handlers 或 Accepter，处理任务队列中具体的逻辑或者建立连接socket。

三种实现：

1. 单 Reactor 单线程

    一个主反应堆 reactor，一个 accepter 或者 handler 来处理接收的事件。

2. 单 Reactor 多线程

    一个主反应堆 reactor 和一个线程池，线程池用来处理分发的事件

3. 主从 Reactor 多线程

    半同步/半反应堆模型。主线程和子线程分工明确，主线程只负责接收新连接，子线程负责完成后续的业务处理，主线程和子线程的交互很简单，主线程只需要把新连接传给子线程，子线程无须返回数据，直接就可以在子线程将处理结果发送给客户端。

**Proactor 模式**

在 Reactor 模式中，Reactor 等待某个事件或者可应用或者操作的状态发生（比如文件描述符可读写，或者是 Socket 可读写）。 然后把这个事件传给事先注册的 Handler（事件处理函数或者回调函数），由后者来做实际的读写操作。 其中的读写操作都需要应用程序同步操作，所以 Reactor 是非阻塞同步网络模型。 如果把 I/O 操作改为异步，即交给操作系统来完成就能进一步提升性能，这就是异步网络模型 Proactor。

# WebServer 项目

项目描述：使用 C++11 编写的 Web 服务器，可以解析GET 和 HEAD 请求，处理静态资源，并实现了异步日志。

主要工作：
•    利用epoll  边沿触发的I/O  多路复用技术，线程池技术，Reactor 模式实现了多线程高并发模型；
•    利用有限状态机解析HTTP 请求报文，实现处理静态资源的请求；
•    使用基于小根堆实现的定时器，关闭超时的非活动连接；
•    使用双缓冲区技术实现异步的日志系统，记录服务器运行状态

## 线程池

线程池类设计要点：

- 如何定义线程需要去执行的“任务”（Tasks）：
    - 首先要明确的一点是，“任务”是函数。无论你是使用Linux C的pthread库，还是C++的库，创建一个线程，为了让线程工作，都需要向线程传递一个函数。
    - 然而，这个函数是需要预先声明和定义的。但是在线程池工作的时候，如果我们想要线程执行不同功能的函数，不可能提前知道并声明和定义好。所以，为了让线程执行不同的函数，传递给线程的函数中，会调用其他的函数以实现执行正确的任务；
- 选择合适的容器：对于线程，数组这类容器就能满足要求；而对于工作队列（Task Queue），可以选择队列或者链表。
- 正确的上锁：本质上，线程池类可以简化成**生产者/消费者**模型。线程执行task是在消耗资源，往线程池中添加task是在生产资源。这里的资源指的就是需要执行的函数。生产者/消费者模型中，有好几种上锁的方式：互斥锁+条件变量、信号量等。选择一种你习惯使用的就行。



**主要组成部分是三个：**

1. 任务队列，存储需要分配的任务，用队列实现

2. 工作线程。

    工作线程不停地读取任务队列，读取里面的任务并处理。

    如果队列为空，工作线程将会被阻塞，条件变量

    队列不为空，唤醒线程，工作

3. 管理线程。

    周期性的对任务队列中的任务数量以及处于忙碌状态的工作线程个数进行检测，当任务过多就适当创建一些新线程，当任务过少就销毁一些线程。

## 并发模型

MainReactor 只有一个，负责响应 client 的连接请求，并建立连接，它使用一个 NIO Selector。在建立连接后用Round Robin的方式分配给某个 SubReactor,因为涉及到跨线程任务分配，需要加锁，这里的锁由某个特定线程中的 loop 创建，只会被该线程和主线程竞争。

SubReactor 可以有一个或者多个，每个 subReactor 都会在一个独立线程中运行，并且维护一个独立的 NIO Selector。

当主线程把新连接分配给了某个 SubReactor，该线程此时可能正阻塞在多路选择器(epoll)的等待中，怎么得知新连接的到来呢？这里使用了 eventfd 进行异步唤醒，线程会从 epoll_wait 中醒来，得到活跃事件，进行处理。

## 定时器

每个SubReactor持有一个定时器，用于处理超时请求和长时间不活跃的连接。muduo中介绍了时间轮的实现和用stl里set的实现，这里我的实现直接使用了stl里的priority_queue，底层是小根堆，并采用惰性删除的方式，时间的到来不会唤醒线程，而是每次循环的最后进行检查，如果超时了再删，因为这里对超时的要求并不会很高，如果此时线程忙，那么检查时间队列的间隔也会短，如果不忙，也给了超时请求更长的等待时间。

## 日志

Log的实现了学习了muduo，Log的实现分为前端和后端，前端往后端写，后端往磁盘写。为什么要这样区分前端和后端呢？因为只要涉及到IO，无论是网络IO还是磁盘IO，肯定是慢的，慢就会影响其它操作，必须让它快才行。

这里的Log前端是前面所述的IO线程，负责产生log，后端是Log线程，设计了多个缓冲区，负责收集前端产生的log，集中往磁盘写。这样，Log写到后端是没有障碍的，把慢的动作交给后端去做好了。

后端主要是由多个缓冲区构成的，集满了或者时间到了就向文件写一次。采用了muduo介绍了“双缓冲区”的思想，实际采用4个多的缓冲区(为什么说多呢？为什么4个可能不够用啊，要有备无患)。4个缓冲区分两组，每组的两个一个主要的，另一个防止第一个写满了没地方写，写满或者时间到了就和另外两个交换**指针**，然后把满的往文件里写。

## 核心结构

- Channel类：Channel是Reactor结构中的“事件”，它自始至终都属于一个EventLoop，负责一个文件描述符的IO事件，在Channel类中保存这IO事件的类型以及对应的回调函数，当IO事件发生时，最终会调用到Channel类中的回调函数。因此，程序中所有带有读写时间的对象都会和一个Channel关联，包括loop中的eventfd，listenfd，HttpData等。
- EventLoop：One loop per thread意味着每个线程只能有一个EventLoop对象，EventLoop即是时间循环，每次从poller里拿活跃事件，并给到Channel里分发处理。EventLoop中的loop函数会在最底层(Thread)中被真正调用，开始无限的循环，直到某一轮的检查到退出状态后从底层一层一层的退出。

# 协程网络库

•    基于 glibc  的 uncontext 函数集的上下文切换实现了 M:N  调度模型下的线程/协程调度器；
•    参考STL 二级空间配置器设计，实现了内存池与协程对象池；
•    将异步处理流程隐藏在底层框架中，上层用户只需使用同步开发的方式就可以获得异步的高性能;
•    对原生socket 的API  进行了类 hook 处理，作了协程化改造；

## 什么是协程

协程就是**用户态线程**，其调度完全由开发者进行控制，因此实现协程的关键就是需要**实现一个用户态线程的调度器**，协程是在用户态中实现的调度，因此避免了内核态的上下文切换造成了性能损失，突破了线程了在 I/O 上的性能瓶颈。

优点：

- 相比于函数:协程避免了传统的函数调用栈，几乎可以无限地递归
- 相比与线程:协程没有内核态的上下文切换，近乎可以无限并发。协程在用户态进程显式的调度，可以把异步操作转换为同步操作，也意味着不需要加锁,避免了加锁过程中不必要的开销。

协程必须支持**挂起/恢复**，因此对于挂起点状态的保存就显得及其关键，我们知道，线程在切换时，它的中断状态会保存在调用栈中。事实上，协程的中断状态也可以通过开辟相应的调用栈来保存。因此，**按照是否开辟相应的调用栈**，我们可以将协程分为两类：

- 有栈协程（Stackful Coroutine）：每个协程都有自己的调用栈，类似于线程的调用栈；（libco、go协程）
- 无栈协程（Stackless Coroutine）：协程中没有自己的调用栈，挂起点的状态通过状态机或闭包来实现。（C++20 中的 cooroutine）

在调度的过程中，根据携程调度权的目标，可以分为两种：

- 对称协程（Symmetric Coroutine）：任一个协程都是相互独立且平等的，调度权可以在任意协程之间转移。
- 非对称协程（Symmetric Coroutine）：协程让出调度权的目标只能是它的调用者，即协程之间存在调用和被调用的关系。

## 进程、线程、协程的区别

**进程**是操作系统进行资源分配的基本单位，每个进程都有自己的独立内存空间。由于进程比较重量，占据独立的内存，所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大，但相对比较稳定安全。

**线程**又叫做轻量级进程，是进程的一个实体，是处理器任务调度和执行的基本单位位。它是比进程更小的能独立运行的基本单位。线程只拥有一点在运行中必不可少的资源(如程序计数器，一组寄存器和栈)，但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。

**协程**，又称微线程，是一种用户态的轻量级线程，协程的调度完全由用户控制（也就是在用户态执行）。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到线程的堆区，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。

协程最大的优势就是协程极高的执行效率。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和线程切换相比，线程数量越多，协程的性能优势就越明显。不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。此外，一个线程的内存在MB级别，而协程只需要KB级别。

- 一个线程可以有多个协程；
- 大多数业务场景下， 线程进程可以看作是同步机制，协程是异步的；
- 线程是抢占式，而协程是非抢占式的，所以需要用户代码释放使用权来切换到其他协程，因此同一时间其实只有一个协程拥有运行权，相当于单线程的能力；
- 协程并不是取代线程，而且抽象于线程之上。线程是被分割的CPU资源, 协程是组织好的代码流程, 协程需要线程来承载运行。

## 协程切换的代价为什么比线程低？

- 协程切换**完全在用户空间进行**，线程切换涉及**特权模式切换，需要在内核空间完成**；
- 协程切换相比线程切换**做的事情更少**。

协程切换只涉及基本的 **CPU 上下文切换**，所谓的 CPU 上下文，就是一堆寄存器，里面保存了 CPU运行任务所需要的信息：从哪里开始运行（%rip：指令指针寄存器，标识 CPU 运行的下一条指令），栈顶的位置（**%rsp：** 是堆栈指针寄存器，通常会指向栈顶位置），当前栈帧在哪（%rbp 是栈帧指针，用于标识当前栈帧的起始位置）以及其它的CPU的中间状态或者结果（%rbx，%r12，%r13，%14，%15 等等）。协程切换非常简单，就是把**当前协程的 CPU 寄存器状态保存起来，然后将需要切换进来的协程的 CPU 寄存器状态加载的 CPU 寄存器上**就可以了。

系统内核调度的对象是线程，因为线程是调度的基本单元（进程是资源拥有的基本单元，进程的切换需要做的事情更多，这里占时不讨论进程切换），而**线程的调度只有拥有最高权限的内核空间才可以完成**，所以线程的切换涉及到**用户空间和内核空间的切换**，也就是特权模式切换，然后需要操作系统调度模块完成**线程调度（task***struct），*而且除了和协程相同基本的 CPU 上下文，还有线程私有的栈和寄存器等，说白了就是上下文比协程多一些，其实简单比较下 task_strcut 和 任何一个协程库的 coroutine 的 struct 结构体大小就能明显区分出来。

## 调度器

netco 会根据计算机的核心数开对应的线程数运行协程，其中每一个线程对应一个 Processor 实例，协程 Coroutine 实例运行在 Processor 的主循环中，Processor 使用 epoller 和定时器 timer 进行任务调度。而 Scheduler 则并不存在一个循环，它是一个全局单例，当某个线程中调用 `co_go()` 运行一个新协程后，实际会调用该实例的方法，选择一个协程最少的 Processor 接管新的协程，当然，用户也可以指定具体某一个 Processor 来接管新的协程。

## context

context 类封装了 ucontext 上下文切换的操作，其他需要使用上下文切换的地方都使用 context 类，目的是将来想使用其他库的上下文切换方法是，只需要实现该类中的方法即可，主要实现了四个方法。

```c++
//函数指针设置当前context的上下文入口
void makeContext(void (*func)(), Processor*, Context*);

//直接用当前程序状态设置当前context的上下文
void makeCurContext();

//将当前上下文保存到oldCtx中，然后切换到当前上下文，若oldCtx为空，则直接运行
void swapToMe(Context* pOldCtx);

//获取当前上下文的ucontext_t指针
inline struct ucontext_t* getUCtx() { return &ctx_; };
```

## Coroutine

协程对象，主要实现协程的几个关键方法：`resume()`，`yield()`，但真正的 `yield` 由 Processor 执行，这里的 `yield` 只是修改当前协程的状态方便在 Processor 中执行。

## Epoller

功能有两个：

- 监视 epoll 中是否有事件发生；
- 向 epoll 中添加、修改、删除监视的 fd。值得注意的是，该类并不存储任何协程对象实体，也不维护任何协程对象实体的生命期，使用的是 LT。

初始 `vector` 长度设置为 16。

`epoll_create1()` 设置 `EPOLL_CLOEXEC`

原因：

子进程以写时复制（Copy-On-Write）方式获得父进程的数据空间、堆和栈副本，这其中也包括文件描述符。刚刚fork 成功时，父子进程中相同的文件描述符指向系统文件表中的同一项（这也意味着他们共享同一文件偏移量）。设置后这样进行 `fork()` 后子进程不能使用父进程中的 fd，父进程可以正常使用，进程替换时会自动关闭文件描述符。

## 对象池

参考 STL 二级空间配置器的设计，对象池主要用于创建 coroutine 实例上，对象池每次创建对象时，会先从**内存池**中取出相应大小的块，内存池与对象大小强相关的，有一个空闲链表，每次分配空间都从空间链表中取，如果空闲链表没有内容，首先会分配 `(分配次数+40)*对象大小` 的空间，然后分成一个个的块，挂到空闲链表上，这里空闲链表节点没有使用额外的空间：效仿的 stl 的二级配置器中的方法，将数据和 `next` 指针放在了一个 `union` 中。从内存池取出所需内存块后，会判断对象是否拥有 non-trivial （显式定义默认构造、拷贝构造、... ）构造函数，没有的话直接返回，有的话使用 placement new 构造对象。

## Timer

定时器主要使用的 Linux 的 `timerfd_create` 创建的时钟 fd 配合一个优先队列（小根堆）实现的，原因是要求效率而没有移除协程的需求。小根堆中存放的是时间和协程对象的 pair：`std::priority_queue<std::pair<Time, Coroutine*>`。

```c++
//获取所有已经超时的需要执行的函数
void getExpiredCoroutines(std::vector<Coroutine*>& expiredCoroutines);
//在time时刻需要恢复协程pCo
void runAt(Time time, Coroutine* pCo);
//经过time毫秒恢复协程pCo
void runAfter(Time time, Coroutine* pCo);
void wakeUp();
//给timefd重新设置时间，time是绝对时间
bool resetTimeOfTimefd(Time time);
```

首先，初始化一个 `timefd_create` 一个 timefd，然后将它放入 epoll 中，如果调用 `runAt` 或 `runAfter` 时，先把新来的任务插入到小根堆中，判断是否是最近的任务，是的话调用 `resetTimeOfTimefd` 来更新时间，如果出现超时，`epoll_wait()` 会跳出阻塞，在 Processor 的主循环中首先处理的就是超时事件，方法就是与当前时间对比并取出小根堆中的协程，直到小根堆中所有任务的时间都比当前大，另外，取出来的协程会放在一个数组中，用于在 Processor 循环中执行。

定时器还有另外一个功能，就是唤醒 `epoll_wait()`，当有新的协程加入时，实际就是通过定时器来唤醒的 processor 主循环，并执行新接受的协程。

## processor

```c++
std::queue<Coroutine*> newCoroutines_[2];
// newCoroutines_为双缓冲队列，一个队列存放新来的协程，另一个给Processor主循环用于执行新来的协程，执行完后就交换队列，每加入一个新的协程就会唤醒一次Processor主循环，以立即执行新来的协程。

std::vector<Coroutine*> actCoroutines_;
// 存放EventEpoller发现的活跃事件的队列，当epoll_wait被激活时，Processor主循环会尝试从Epoller中获取活跃的协程，存放在actCoroutine_队列中，然后依次恢复执行。

std::vector<Coroutine*> timerExpiredCo_;
// 存放超时的协程队列，当epoll_wait被激活时，Processor主循环会首先尝试从Timer中获取活跃的协程，存放在timerExpiredCo队列中，然后依次恢复执行。

std::vector<Coroutine*> removedCo_;
// 存放被移除的协程列表，要移除某一个事件会先放在该列表中，一次循环结束才会真正delete
```

执行顺序：

执行超时的协程(`timerExpiredCo_`) ---> 执行新来的协程(`newCoroutines_`) ---> 执行 epoller 中被激活的协程(`actCoroutines_`) ----> 清理 `removerdCo_` 中的协程。

## Scheduler

调度器，指协程应该运行在哪个 Processor 上，netco 中的该类为全局单例，所执行的调度也相对比较简单，其可以让用户指定协程运行在某个 Processor 上，若用户没有指定，则挑选协程数量最少的 Processor 接管新的协程。

在 libgo 和 Golang 中，scheduler 还有一个 steal 的操作，可以将一个协程从一个 Processor 中偷到另一个 Processor 中，因为其 Processor 的主循环是允许阻塞的，并且协程的运行完全由库决定。而 netco 可以让用户指定某个协程一直运行在某个 Processor 上。
