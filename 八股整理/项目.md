自我介绍：面试官，您好，我是徐秋晨，目前是电子科技大学研二的一名学生，专业是通信工程。

主要做了两个和C++网络编程的项目：

第一个项目是在学习网络编程和在github中参考其他优秀的开源项目中完成的基于C++11实现的多线程web服务器，解析http请求，并发模型是基于主从reactor模型和I/O多路复用epoll实现的，MainReactor 只有一个，负责响应 client 的连接请求，并建立连接，它使用一个NIO Selector。在建立连接后用轮询的方式分配给某个SubReactor，因为涉及到跨线程任务分配，需要加锁，这里的锁由某个特定线程中的loop创建，只会被该线程和主线程竞争。SubReactor可以有一个或者多个，每个subReactor都会在一个独立线程中运行，并且维护一个独立的NIO Selector。当主线程把新连接分配给了某个SubReactor，该线程此时可能正阻塞在多路选择器(epoll)的等待中，使用了eventfd进行异步唤醒，线程会从epoll_wait中醒来，得到活跃事件，进行处理。后端日志主要是由多个缓冲区构成的，参考了muduo介绍了“双缓冲区”的思想，一个主要的，另一个防止第一个写满了没地方写，写满或者时间到了就和另外两个交换**指针**，然后把满的往文件里写。

第二项目是通过调研C++中的开源协程库和Golang中的协程模型设计的基于C++11  的轻量级对称协程网络库，协程是轻量级的线程实现，其调度完全由开发者进行控制，协程是在用户态中实现的调度，相比于线程，避免了内核态的上下文切换造成了性能损失，协程的切换需要保存当前的上下文，首先是参考了云风的coroutine的协程库，使用了 glibc 中的uncontext 函数集实现了上下文的协程，封装了协程对象和协程调度器，其中每一个线程对应一个 Processor 实例，协程 Coroutine 实例运行在Processor的主循环中，Processor使用epoll和定时器timer进行任务调度。而Scheduler则并不存在一个循环，它是一个全局单例，当某个线程中调用co_go()运行一个新协程后，实际会调用该实例的方法，选择一个协程最少的Processor接管新的协程，当然，用户也可以指定具体某一个Processor来接管新的协程；同时参考了STL二级空间配置器的设计，实现了对象池，参考 STL 二级空间配置器的设计，对象池每次创建对象时，会先从**内存池**中取出相应大小的块，内存池与对象大小强相关的，有一个空闲链表，每次分配空间都从空间链表中取，如果空闲链表没有内容，首先会分配 `(分配次数+40)*对象大小` 的空间，然后分成一个个的块，挂到空闲链表上。从内存池取出所需内存块后，会判断对象是否拥有 non-trivial （显式定义默认构造、拷贝构造、... ）构造函数，没有的话直接返回，有的话使用 placement new 构造对象。同时对原生的socket进行了协程化改造和是使用C++11中stomic实现了自旋锁，在测试后相比于上一个主从reactor模型QPS要提升1.5倍左右。

# 网络编程

## I/O 模型

- 阻塞 I/O
- 非阻塞 I/O
- I/O 多路复用（select, poll, epoll）
- 异步 I/O

## I/O 多路复用

### **select/poll**

select 函数将许多个文件描述符集中到一起进行监视，使用 fd_set 数组保存被监视的文件描述符的变化，这个数组是以位存储在内核中的。即该数组所在的索引就是对应文件描述符的id。

select 实现多路复用的方式是，将已连接的 Socket 都放到一个**文件描述符集合**，然后调用 select 函数将文件描述符集合**拷贝**到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过**遍历**文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合**拷贝**回用户态里，然后用户态还需要再通过**遍历**的方法找到可读或可写的 Socket，然后再对其处理。

对于 select 这种方式，需要进行 **2 次「遍历」文件描述符集合**，一次是在内核态里，一个次是在用户态里 ，而且还会发生 **2 次「拷贝」文件描述符集合**，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。

select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 `1024`，只能监听 0~1023 的文件描述符；poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。

两种方式没有本质的区别，**都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长。

### **epoll**

epoll 的函数：

```cpp
int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout);
```

1. 调用epoll_create建立一个epoll对象(在epoll文件系统中给这个句柄分配资源)；
2. 调用epoll_ctl向epoll对象中添加套接字；
3. 调用epoll_wait收集发生事件的连接。

epoll 高效的原因主要是 epoll_wait 这个函数。由于我们在调用 epoll_create 时，内核除了帮我们在 epoll 文件系统里建了个 file 结点，在内核 cache 里建了个红黑树用于存储以后 epoll_ctl 传来的 socket 外，还会再建立一个 list 链表，用于存储准备就绪的事件，当 epoll_wait 调用时，仅仅观察这个 list 链表里有没有数据即可。有数据就返回，没有数据就 sleep，等到 timeout 时间到后即使链表没数据也返回。所以，epoll_wait 非常高效。而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句柄而已，所以，epoll_wait 仅需要从内核态 copy 少量的文件描述符到用户态。

**水平触发和边缘触发**

- 在LT（水平触发）模式下，只要这个文件描述符还有数据可读，每次 epoll_wait 都会返回它的事件，提醒用户程序去操作。
- ET（边缘触发）模式下，在它检测到有 I/O 事件时，通过 epoll_wait 调用会得到有事件通知的文件描述符，**对于每一个被通知的文件描述符，如可读，则必须将该文件描述符一直读到空**，让 errno 返回 EAGAIN 为止，否则下次的 epoll_wait 不会返回余下的数据，会丢掉事件。如果 ET 模式不是非阻塞的，那这个一直读或一直写势必会在最后一次阻塞。

### epoll 是同步还是异步的？

- 从 IO 层面来看，epoll 绝对是同步的；
- 从消息处理层面来看，epoll 是异步的.

select，poll，epoll 本质上都是同步 I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的（可能通过 while 循环来检测内核将数据准备的怎么样了， 而不是属于内核的一种通知用户态机制），仍然需要 read、write 去读写数据。

用户线程定期轮询 epoll 文件描述符上的事件，事件发生后，读取事件对应的 epoll_data，该结构中包含了文件fd和数据地址，由于采用了mmap，程序可以直接读取数据（epoll_wait 函数）。有人把 epoll 这种方式叫做同步非阻塞（NIO），因为用户线程需要不停地轮询，自己读取数据，看上去好像只有一个线程在做事情。也有人把这种方式叫做异步非阻塞（AIO），因为毕竟是内核线程负责扫描 fd 列表，并填充事件链表的。个人认为真正理想的异步非阻塞，应该是内核线程填充事件链表后，主动通知用户线程，或者调用应用程序事先注册的回调函数来处理数据，如果还需要用户线程不停的轮询来获取事件信息，就不是太完美了，所以也有不少人认为 epoll 是伪 AIO，还是有道理的。

## 两种处理事件方式

**Reactor 模式**

**定义：**是一种接收多个输入事件的服务器事件驱动处理模式。服务器端通过 IO 多路复用来处理这些输入事件，并将这些事件同步分派给对应的处理线程。Reactor 模式本质上就是将收到的事件进行分发处理。Reactor 模式中有两个关键的组成：

- 主反应堆 Reactor 在一个单独的线程中运行，负责监听和分发事件，将接收到的事件分为监听 socket 和连接 socket，连接 socket 放入任务队列让线程池线程去抢占式调度。

- Handlers 或 Accepter，处理任务队列中具体的逻辑或者建立连接socket。

三种实现：

1. 单 Reactor 单线程

    一个主反应堆 reactor，一个 accepter 或者 handler 来处理接收的事件。

2. 单 Reactor 多线程

    一个主反应堆 reactor 和一个线程池，线程池用来处理分发的事件

3. 主从 Reactor 多线程

    半同步/半反应堆模型。主线程和子线程分工明确，主线程只负责接收新连接，子线程负责完成后续的业务处理，主线程和子线程的交互很简单，主线程只需要把新连接传给子线程，子线程无须返回数据，直接就可以在子线程将处理结果发送给客户端。

**Proactor 模式**

在 Reactor 模式中，Reactor 等待某个事件或者可应用或者操作的状态发生（比如文件描述符可读写，或者是 Socket 可读写）。 然后把这个事件传给事先注册的 Handler（事件处理函数或者回调函数），由后者来做实际的读写操作。 其中的读写操作都需要应用程序同步操作，所以 Reactor 是非阻塞同步网络模型。 如果把 I/O 操作改为异步，即交给操作系统来完成就能进一步提升性能，这就是异步网络模型 Proactor。

# WebServer 项目

项目描述：使用 C++11 编写的 Web 服务器，可以解析GET 和 HEAD 请求，处理静态资源，并实现了异步日志。

主要工作：
•    利用epoll  边沿触发的I/O  多路复用技术，线程池技术，Reactor 模式实现了多线程高并发模型；
•    利用有限状态机解析HTTP 请求报文，实现处理静态资源的请求；
•    使用基于小根堆实现的定时器，关闭超时的非活动连接；
•    使用双缓冲区技术实现异步的日志系统，记录服务器运行状态

## 并发模型

MainReactor 只有一个，负责响应 client 的连接请求，并建立连接，它使用一个NIO Selector。在建立连接后用Round Robin的方式分配给某个SubReactor,因为涉及到跨线程任务分配，需要加锁，这里的锁由某个特定线程中的loop创建，只会被该线程和主线程竞争。

SubReactor可以有一个或者多个，每个subReactor都会在一个独立线程中运行，并且维护一个独立的NIO Selector。

当主线程把新连接分配给了某个SubReactor，该线程此时可能正阻塞在多路选择器(epoll)的等待中，怎么得知新连接的到来呢？这里使用了eventfd进行异步唤醒，线程会从epoll_wait中醒来，得到活跃事件，进行处理。

## 定时器

每个SubReactor持有一个定时器，用于处理超时请求和长时间不活跃的连接。muduo中介绍了时间轮的实现和用stl里set的实现，这里我的实现直接使用了stl里的priority_queue，底层是小根堆，并采用惰性删除的方式，时间的到来不会唤醒线程，而是每次循环的最后进行检查，如果超时了再删，因为这里对超时的要求并不会很高，如果此时线程忙，那么检查时间队列的间隔也会短，如果不忙，也给了超时请求更长的等待时间。

## 日志

Log的实现了学习了muduo，Log的实现分为前端和后端，前端往后端写，后端往磁盘写。为什么要这样区分前端和后端呢？因为只要涉及到IO，无论是网络IO还是磁盘IO，肯定是慢的，慢就会影响其它操作，必须让它快才行。

这里的Log前端是前面所述的IO线程，负责产生log，后端是Log线程，设计了多个缓冲区，负责收集前端产生的log，集中往磁盘写。这样，Log写到后端是没有障碍的，把慢的动作交给后端去做好了。

后端主要是由多个缓冲区构成的，集满了或者时间到了就向文件写一次。采用了muduo介绍了“双缓冲区”的思想，实际采用4个多的缓冲区(为什么说多呢？为什么4个可能不够用啊，要有备无患)。4个缓冲区分两组，每组的两个一个主要的，另一个防止第一个写满了没地方写，写满或者时间到了就和另外两个交换**指针**，然后把满的往文件里写。

## 核心结构

- Channel类：Channel是Reactor结构中的“事件”，它自始至终都属于一个EventLoop，负责一个文件描述符的IO事件，在Channel类中保存这IO事件的类型以及对应的回调函数，当IO事件发生时，最终会调用到Channel类中的回调函数。因此，程序中所有带有读写时间的对象都会和一个Channel关联，包括loop中的eventfd，listenfd，HttpData等。
- EventLoop：One loop per thread意味着每个线程只能有一个EventLoop对象，EventLoop即是时间循环，每次从poller里拿活跃事件，并给到Channel里分发处理。EventLoop中的loop函数会在最底层(Thread)中被真正调用，开始无限的循环，直到某一轮的检查到退出状态后从底层一层一层的退出。

# 协程网络库

•    基于glibc  的uncontext 函数集的上下文切换实现了M:N  调度模型下的线程/协程调度器；
•    参考STL 二级空间配置器设计，实现了内存池与协程对象池；
•    将异步处理流程隐藏在底层框架中，上层用户只需使用同步开发的方式就可以获得异步的高性能;
•    对原生socket 的API  进行了类 hook 处理，作了协程化改造；

## 什么是协程

协程就是**用户态线程**，其调度完全由开发者进行控制，因此实现协程的关键就是需要**实现一个用户态线程的调度器**，协程是在用户态中实现的调度，因此避免了内核态的上下文切换造成了性能损失，突破了线程了在 I/O 上的性能瓶颈。

优点：

- 相比于函数:协程避免了传统的函数调用栈，几乎可以无限地递归
- 相比与线程:协程没有内核态的上下文切换，近乎可以无限并发。协程在用户态进程显式的调度，可以把异步操作转换为同步操作，也意味着不需要加锁,避免了加锁过程中不必要的开销。

协程必须支持**挂起/恢复**，因此对于挂起点状态的保存就显得及其关键，我们知道，线程在切换时，它的中断状态会保存在调用栈中。事实上，协程的中断状态也可以通过开辟相应的调用栈来保存。因此，**按照是否开辟相应的调用栈**，我们可以将协程分为两类：

- 有栈协程（Stackful Coroutine）：每个协程都有自己的调用栈，类似于线程的调用栈；（libco、go协程）
- 无栈协程（Stackless Coroutine）：协程中没有自己的调用栈，挂起点的状态通过状态机或闭包来实现。（C++20 中的 cooroutine）

在调度的过程中，根据携程调度权的目标，可以分为两种：

- 对称协程（Symmetric Coroutine）：任一个协程都是相互独立且平等的，调度权可以在任意协程之间转移。
- 非对称协程（Symmetric Coroutine）：协程让出调度权的目标只能是它的调用者，即协程之间存在调用和被调用的关系。

## 调度器

netco 会根据计算机的核心数开对应的线程数运行协程，其中每一个线程对应一个 Processor 实例，协程 Coroutine 实例运行在Processor的主循环中，Processor使用epoll和定时器timer进行任务调度。而Scheduler则并不存在一个循环，它是一个全局单例，当某个线程中调用co_go()运行一个新协程后，实际会调用该实例的方法，选择一个协程最少的Processor接管新的协程，当然，用户也可以指定具体某一个Processor来接管新的协程。

## 对象池

参考 STL 二级空间配置器的设计，对象池主要用于创建 coroutine 实例上，对象池每次创建对象时，会先从**内存池**中取出相应大小的块，内存池与对象大小强相关的，有一个空闲链表，每次分配空间都从空间链表中取，如果空闲链表没有内容，首先会分配 `(分配次数+40)*对象大小` 的空间，然后分成一个个的块，挂到空闲链表上，这里空闲链表节点没有使用额外的空间：效仿的 stl 的二级配置器中的方法，将数据和 `next` 指针放在了一个 `union` 中。从内存池取出所需内存块后，会判断对象是否拥有 non-trivial （显式定义默认构造、拷贝构造、... ）构造函数，没有的话直接返回，有的话使用 placement new 构造对象。

## 自旋锁

单看使用方法和使用互斥量的代码是差不多的。只不过自旋锁不会引起线程休眠。当共享资源的状态不满足的时候，自旋锁会不停地循环检测状态。因为不会陷入休眠，而是忙等待的方式也就不需要条件变量。

原子操作 `atomic` 

不休眠就不会引起上下文切换，但是会比较浪费CPU。
